{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Optional\n",
        "from collections import Counter\n",
        "import os\n",
        "import csv\n",
        "!pip install torchmetrics\n",
        "!pip install pytorch-metric-learning\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "!pip install pytorch-lightning\n",
        "import torch.optim as optim\n",
        "import torchmetrics\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:00.449032Z",
          "iopub.execute_input": "2022-02-17T08:25:00.449347Z",
          "iopub.status.idle": "2022-02-17T08:25:00.482131Z",
          "shell.execute_reply.started": "2022-02-17T08:25:00.449262Z",
          "shell.execute_reply": "2022-02-17T08:25:00.481441Z"
        },
        "trusted": true,
        "id": "SWWjBbIZ5LSi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e766c1af-1464-4ecc-80ad-bddafc3008fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-metric-learning\n",
            "  Downloading pytorch_metric_learning-2.0.1-py3-none-any.whl (109 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.3/109.3 KB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from pytorch-metric-learning) (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pytorch-metric-learning) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from pytorch-metric-learning) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-metric-learning) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (4.5.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->pytorch-metric-learning) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->pytorch-metric-learning) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->pytorch-metric-learning) (1.1.1)\n",
            "Installing collected packages: pytorch-metric-learning\n",
            "Successfully installed pytorch-metric-learning-2.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.0.0-py3-none-any.whl (715 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m715.6/715.6 KB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (1.13.1+cu116)\n",
            "Collecting lightning-utilities>=0.7.0\n",
            "  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (2023.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (0.11.4)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.25.1)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (22.2.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4.0,>=2.0\n",
            "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.15)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n",
            "Installing collected packages: multidict, lightning-utilities, frozenlist, charset-normalizer, async-timeout, yarl, aiosignal, aiohttp, pytorch-lightning\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 charset-normalizer-3.1.0 frozenlist-1.3.3 lightning-utilities-0.8.0 multidict-6.0.4 pytorch-lightning-2.0.0 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Todo Part 1\n",
        "Complete the implementation of the encode method of the Tokenizer class:\n",
        "\n",
        "`encode`: encode a given space-separated text into list of token ids according to the `self.token2idx` property. For tokens not present in the mapping, use the id of the `<unk>` token. If `max_length` is set, pad the input to `max_length` if it is less than `max_length` and truncate to `max_length` if it exceeds the length.\n",
        "\n",
        "Examples\n",
        "```python\n",
        "text = \"hello transformers !\"\n",
        "tokenizer.encode(text)                  # example output: [3, 4, 5]\n",
        "tokenizer.encode(text, max_length=5)    # example output: [3, 4, 5, 0, 0]\n",
        "tokenizer.encode(text, max_length=2)    # example output: [3, 4]\n",
        "```"
      ],
      "metadata": {
        "id": "FF4glogI5LSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        # two special tokens for padding and unknown\n",
        "        self.token2idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "        self.idx2token = [\"<pad>\", \"<unk>\"]\n",
        "        self.is_fit = False\n",
        "    \n",
        "    @property\n",
        "    def pad_id(self):\n",
        "        return self.token2idx[\"<pad>\"]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.idx2token)\n",
        "    \n",
        "    def fit(self, train_texts: List[str]):\n",
        "        counter = Counter()\n",
        "        for text in train_texts:\n",
        "            counter.update(text.lower().split())\n",
        "        \n",
        "        # manually set a vocabulary size for the data set\n",
        "        vocab_size = 20000\n",
        "        self.idx2token.extend([token for token, count in counter.most_common(vocab_size - 2)])\n",
        "        for (i, token) in enumerate(self.idx2token):\n",
        "            self.token2idx[token] = i\n",
        "            \n",
        "        self.is_fit = True\n",
        "    \"\"\"            \n",
        "    def encode(self, text: str, max_length: Optional[int] = None) -> List[int]:\n",
        "        if not self.is_fit:\n",
        "            raise Exception(\"Please fit the tokenizer on the training tokens\")\n",
        "            \n",
        "        # TODO: implement the encode method, the method signature shouldn't be changed\n",
        "        raise NotImplemented\n",
        "    \"\"\"\n",
        "    def encode(self, text: str, max_length: Optional[int] = None) -> List[int]:\n",
        "        if not self.is_fit:\n",
        "            raise Exception(\"Please fit the tokenizer on the training tokens\")\n",
        "            \n",
        "        tokens = text.lower().split()\n",
        "        if max_length is not None:\n",
        "            tokens = tokens[:max_length]\n",
        "        \n",
        "        token_ids = [self.token2idx.get(token, self.token2idx[\"<unk>\"]) for token in tokens]\n",
        "        if max_length is not None and len(token_ids) < max_length:\n",
        "            token_ids += [self.pad_id] * (max_length - len(token_ids))\n",
        "        \n",
        "        return token_ids\n",
        "\n",
        "    def decode(self, token_ids: List[int]) -> str:\n",
        "        tokens = [self.idx2token[token_id] for token_id in token_ids]\n",
        "        text = \" \".join(tokens)#.replace(\"<pad>\", \"\").replace(\"<unk>\", \"\")\n",
        "        return text.strip()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:00.483609Z",
          "iopub.execute_input": "2022-02-17T08:25:00.483925Z",
          "iopub.status.idle": "2022-02-17T08:25:00.494599Z",
          "shell.execute_reply.started": "2022-02-17T08:25:00.483881Z",
          "shell.execute_reply": "2022-02-17T08:25:00.493679Z"
        },
        "trusted": true,
        "id": "u29mNAdI5LSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_raw_data(filepath: str, with_tags: bool = True):\n",
        "    data = {'text': []}\n",
        "    if with_tags:\n",
        "        data['tags'] = []\n",
        "        with open(filepath) as f:\n",
        "            reader = csv.reader(f)\n",
        "            for text, tags in reader:\n",
        "                data['text'].append(text)\n",
        "                data['tags'].append(tags)\n",
        "    else:\n",
        "        with open(filepath) as f:\n",
        "            for line in f:\n",
        "                data['text'].append(line.strip())\n",
        "    return data"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:00.495804Z",
          "iopub.execute_input": "2022-02-17T08:25:00.496201Z",
          "iopub.status.idle": "2022-02-17T08:25:00.504688Z",
          "shell.execute_reply.started": "2022-02-17T08:25:00.496166Z",
          "shell.execute_reply": "2022-02-17T08:25:00.503898Z"
        },
        "trusted": true,
        "id": "7lHbdxRn5LSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# train = files.upload()"
      ],
      "metadata": {
        "id": "vLPfXe6_7I1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_tokens = files.upload()"
      ],
      "metadata": {
        "id": "n_1kxcO37SRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# val = files.upload()"
      ],
      "metadata": {
        "id": "zpWGzQBP7SrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "data_dir = \"./\"\n",
        "train_raw = load_raw_data(os.path.join(data_dir, \"train.csv\"))\n",
        "val_raw = load_raw_data(os.path.join(data_dir, \"val.csv\"))\n",
        "test_raw = load_raw_data(os.path.join(data_dir, \"test_tokens.txt\"), with_tags=False)\n",
        "# fit the tokenizer on the training tokens\n",
        "tokenizer.fit(train_raw['text'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:00.517671Z",
          "iopub.execute_input": "2022-02-17T08:25:00.518378Z",
          "iopub.status.idle": "2022-02-17T08:25:00.795602Z",
          "shell.execute_reply.started": "2022-02-17T08:25:00.518340Z",
          "shell.execute_reply": "2022-02-17T08:25:00.794913Z"
        },
        "trusted": true,
        "id": "dEuoJh1Q5LSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#upload the dataset\n",
        "#for google colb, use this\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ],
      "metadata": {
        "id": "94u1vPV-lbXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2d7nueEY_wrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NERDataset: \n",
        "    tag2idx = {'O': 1, 'B-PER': 2, 'I-PER': 3, 'B-ORG': 4, 'I-ORG': 5, 'B-LOC': 6, 'I-LOC': 7, 'B-MISC': 8, 'I-MISC': 9}\n",
        "    idx2tag = ['<pad>', 'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG','B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
        "  \n",
        "    def __init__(self, raw_data: Dict[str, List[str]], tokenizer: Tokenizer, max_length: int = 128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.token_ids = []\n",
        "        self.tag_ids = []\n",
        "        self.with_tags = False\n",
        "        for text in raw_data['text']:\n",
        "            self.token_ids.append(tokenizer.encode(text, max_length=max_length))\n",
        "        if 'tags' in raw_data:\n",
        "            self.with_tags = True\n",
        "            for tags in raw_data['tags']:\n",
        "                self.tag_ids.append(self.encode_tags(tags, max_length=max_length))\n",
        "    \n",
        "    def encode_tags(self, tags: str, max_length: Optional[int] = None):\n",
        "        tag_ids = [self.tag2idx[tag] for tag in tags.split()]\n",
        "        if max_length is None:\n",
        "            return tag_ids\n",
        "        # truncate the tags if longer than max_length\n",
        "        if len(tag_ids) > max_length:\n",
        "            return tag_ids[:max_length]\n",
        "        # pad with 0s if shorter than max_length\n",
        "        else:\n",
        "            return tag_ids + [0] * (max_length - len(tag_ids))  # 0 as padding for tags\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.token_ids)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        token_ids = torch.LongTensor(self.token_ids[idx])\n",
        "        mask = token_ids == self.tokenizer.pad_id  # padding tokens\n",
        "        if self.with_tags:\n",
        "            # for training and validation\n",
        "            return token_ids, mask, torch.LongTensor(self.tag_ids[idx])\n",
        "        else:\n",
        "            # for testing\n",
        "            return token_ids, mask\n",
        "        "
      ],
      "metadata": {
        "id": "KzUsGMealyZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_data = NERDataset(train_raw, tokenizer)\n",
        "va_data = NERDataset(val_raw, tokenizer)\n",
        "te_data = NERDataset(test_raw, tokenizer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:02.109558Z",
          "iopub.execute_input": "2022-02-17T08:25:02.109921Z",
          "iopub.status.idle": "2022-02-17T08:25:02.467151Z",
          "shell.execute_reply.started": "2022-02-17T08:25:02.109883Z",
          "shell.execute_reply": "2022-02-17T08:25:02.466435Z"
        },
        "trusted": true,
        "id": "0kMIKu-p5LSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test1 = \"my last name is marks i am from san jose california\"\n",
        "test2 = tokenizer.decode(tokenizer.encode(test1))\n",
        "print(test2)\n",
        "test3 = \"gonna fly now\"\n",
        "print(tokenizer.encode(test3, max_length = 5))\n",
        "print(tokenizer.decode(tokenizer.encode(test3)))\n",
        "test4 = \"\"\"Look I was gonna go easy on you not to hurt your feelings\n",
        "But I'm only going to get this one chance six minutes six minutes\n",
        "Something s wrong I can feel it six minutes Slim Shady you re on\n",
        "Just a feeling I ve got like something s about to happen but I don t know what\n",
        "If that means what I think it means  we re in trouble big trouble\n",
        "And if he is as bananas as you say I'm not taking any chances\n",
        "You are just what the doc ordered\"\"\"\n",
        "test5 = tokenizer.encode(test4)\n",
        "print(test5)\n",
        "test6 = tokenizer.decode(test5)\n",
        "print(test6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqpQcFx7_T93",
        "outputId": "3f2bee0a-fe35-49fd-8fd2-193c60c41be9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my last name is marks i am from san jose california\n",
            "[1, 2200, 237, 0, 0]\n",
            "<unk> fly now\n",
            "[1673, 65, 20, 1, 367, 3812, 13, 256, 39, 7, 2321, 1360, 1, 37, 1, 143, 446, 7, 442, 61, 71, 1096, 148, 299, 148, 299, 1799, 4657, 8029, 65, 309, 1485, 28, 148, 299, 9101, 1, 256, 7151, 13, 176, 8, 4857, 65, 1, 851, 541, 1799, 4657, 94, 7, 3289, 37, 65, 6491, 1, 925, 313, 141, 25, 2116, 313, 65, 628, 28, 2116, 69, 7151, 6, 3107, 1011, 3107, 9, 141, 26, 30, 32, 1, 32, 256, 377, 1, 39, 469, 220, 3545, 256, 57, 176, 313, 2, 1, 1155]\n",
            "look i was <unk> go easy on you not to hurt your <unk> but <unk> only going to get this one chance six minutes six minutes something s wrong i can feel it six minutes slim <unk> you re on just a feeling i <unk> got like something s about to happen but i don <unk> know what if that means what i think it means we re in trouble big trouble and if he is as <unk> as you say <unk> not taking any chances you are just what the <unk> ordered\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Todo Part 2\n",
        "Implement and experiment with transformer models. The implementation should include **at least** the following:\n",
        "- `nn.Embedding` layer to embed input token ids to the embedding space\n",
        "- `nn.TransformerEncoder` layer to perform transformer operations\n",
        "- `nn.Linear` layer as the output layer to map the output to the number of classes\n",
        "\n",
        "As we will be using the cross-entropy loss, an `nn.Softmax` or `nn.LogSoftmax` layer is not needed.\n",
        "\n",
        "You can refer to the following links for transformer Docs and examples:\n",
        "\n",
        "https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "\n",
        "You can modify the `__init__` method including the signature needed. For the `forward` method, the method signature is given as follows:\n",
        "\n",
        "- `src`: a `torch.LongTensor` of shape (batch_size, max_length, vocab_size) representing the input text tokens.\n",
        "\n",
        "- `src_mask`: a `torch.BoolTensor` of shape (batch_size, max_length) indicating whether an input position is padded. This is needed to prevent the transformer model attending to padded tokens.\n",
        "\n",
        "The output from the `forward` method should be of shape (batch_size, max_length, num_classes). Note that the number of classes should be 10 instead of 9 because of an additional padding class.\n"
      ],
      "metadata": {
        "id": "QVOHqRsD5LSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: implement the Transformer model architecture and forward method\n",
        "# class TransformerModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#     def forward(self, src: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
        "#         raise NotImplemented\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size: int, embed_size: int, num_heads: int, hidden_size: int, num_layers: int, num_classes = 10):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=hidden_size)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.output_layer = nn.Linear(embed_size, num_classes)\n",
        "        \n",
        "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.embedding(src) * torch.sqrt(torch.tensor(self.embedding.embedding_dim, dtype=torch.float32))\n",
        "        x = x.transpose(0, 1)\n",
        "        x = self.encoder(x, src_key_padding_mask=src_mask)\n",
        "        x = x.transpose(0, 1)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:02.468580Z",
          "iopub.execute_input": "2022-02-17T08:25:02.468821Z",
          "iopub.status.idle": "2022-02-17T08:25:02.478006Z",
          "shell.execute_reply.started": "2022-02-17T08:25:02.468786Z",
          "shell.execute_reply": "2022-02-17T08:25:02.477246Z"
        },
        "trusted": true,
        "id": "Jvy2pBQL5LSo"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modify as required\n",
        "def validate(\n",
        "    model: nn.Module, \n",
        "    dataloader: DataLoader, \n",
        "    device: torch.device,\n",
        "):\n",
        "    acc_metric = torchmetrics.Accuracy(task = 'multiclass', num_classes = 10, compute_on_step=False).to(device)\n",
        "    loss_metric = torchmetrics.MeanMetric(compute_on_step=False).to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            input_ids, input_mask, tags = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "            # output shape: (batch_size, max_length, num_classes)\n",
        "            logits = model(input_ids, input_mask)\n",
        "            # ignore padding index 0 when calculating loss\n",
        "            loss = F.cross_entropy(logits.reshape(-1, 10), tags.reshape(-1), ignore_index=0)\n",
        "                \n",
        "            loss_metric.update(loss, input_mask.numel() - input_mask.sum())\n",
        "            is_active = torch.logical_not(input_mask)  # non-padding elements\n",
        "            # only consider non-padded tokens when calculating accuracy\n",
        "            acc_metric.update(logits[is_active], tags[is_active])\n",
        "    \n",
        "    print(f\"| Validate | loss {loss_metric.compute():.4f} | acc {acc_metric.compute():.4f} |\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:03.082423Z",
          "iopub.execute_input": "2022-02-17T08:25:03.082694Z",
          "iopub.status.idle": "2022-02-17T08:25:03.091306Z",
          "shell.execute_reply.started": "2022-02-17T08:25:03.082660Z",
          "shell.execute_reply": "2022-02-17T08:25:03.090373Z"
        },
        "trusted": true,
        "id": "Y5Eaibzu5LSp"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modify as required\n",
        "def train(\n",
        "    model: nn.Module, \n",
        "    dataloader: DataLoader, \n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        "):\n",
        "    acc_metric = torchmetrics.Accuracy(task = 'multiclass', num_classes = 10, compute_on_step=False).to(device)\n",
        "    loss_metric = torchmetrics.MeanMetric(compute_on_step=False).to(device)\n",
        "    model.train()\n",
        "    \n",
        "    # loop through all batches in the training\n",
        "    for batch in tqdm(dataloader):\n",
        "        input_ids, input_mask, tags = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids, input_mask)\n",
        "        loss = F.cross_entropy(logits.reshape(-1, 10), tags.reshape(-1), ignore_index=0)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_metric.update(loss, input_mask.numel() - input_mask.sum())\n",
        "        is_active = torch.logical_not(input_mask)\n",
        "        acc_metric.update(logits[is_active], tags[is_active])\n",
        "    \n",
        "    print(f\"| Epoch {epoch} | loss {loss_metric.compute():.4f} | acc {acc_metric.compute():.4f} |\")\n",
        "    "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:03.092754Z",
          "iopub.execute_input": "2022-02-17T08:25:03.093232Z",
          "iopub.status.idle": "2022-02-17T08:25:03.104319Z",
          "shell.execute_reply.started": "2022-02-17T08:25:03.093195Z",
          "shell.execute_reply": "2022-02-17T08:25:03.103543Z"
        },
        "trusted": true,
        "id": "qQtTOXRA5LSp"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modify as required\n",
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# data loaders\n",
        "train_dataloader = DataLoader(tr_data, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(va_data, batch_size=32)\n",
        "test_dataloader = DataLoader(te_data, batch_size=32)\n",
        "\n",
        "# move the model to device\n",
        "model = TransformerModel(vocab_size = len(tokenizer), \n",
        "    embed_size = 256, \n",
        "    num_heads = 4, \n",
        "    hidden_size = 256,\n",
        "    num_layers = 2,).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "for epoch in range(12):\n",
        "    train(model, train_dataloader, optimizer, device, epoch)\n",
        "validate(model, val_dataloader, device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:03.105602Z",
          "iopub.execute_input": "2022-02-17T08:25:03.105982Z",
          "iopub.status.idle": "2022-02-17T08:25:43.205981Z",
          "shell.execute_reply.started": "2022-02-17T08:25:03.105946Z",
          "shell.execute_reply": "2022-02-17T08:25:43.205084Z"
        },
        "trusted": true,
        "id": "5Be4ZCs15LSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67bbd6a5-781d-47af-d657-9f80d63785fb"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:07<00:00, 58.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 0 | loss 0.5017 | acc 0.8693 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:07<00:00, 57.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 1 | loss 0.2974 | acc 0.9160 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:07<00:00, 58.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 2 | loss 0.2198 | acc 0.9353 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:07<00:00, 62.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 3 | loss 0.1722 | acc 0.9482 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:07<00:00, 58.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 4 | loss 0.1406 | acc 0.9569 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:07<00:00, 62.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 5 | loss 0.1188 | acc 0.9627 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:07<00:00, 59.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 6 | loss 0.1027 | acc 0.9673 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:07<00:00, 62.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 7 | loss 0.0922 | acc 0.9704 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:07<00:00, 59.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 8 | loss 0.0822 | acc 0.9729 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:07<00:00, 60.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 9 | loss 0.0766 | acc 0.9749 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:07<00:00, 61.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 10 | loss 0.0697 | acc 0.9768 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:07<00:00, 58.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 11 | loss 0.0639 | acc 0.9791 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 102/102 [00:00<00:00, 176.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Validate | loss 0.3307 | acc 0.9296 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Todo Part 3\n",
        "Make predictions on the validation data and evaluate entity-level F1 scores using conlleval script.\n",
        "\n",
        "`predict`: taking inputs of a trained model, a dataloader, and a torch device, predict the tags for all tokens in the data set. The output should be a nested list of lists, each containing tag predictions for a single sentence.\n",
        "\n",
        "    Input texts in the dataloader (2 sentences):\n",
        "    EU rejects German call\n",
        "    Only France and Britain backed Fischler 's proposal .\n",
        "    \n",
        "    Example output:\n",
        "    [['B-ORG', 'O', 'B-MISC', 'O'], ['O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-PER', 'O', 'O', 'O']]\n",
        "        "
      ],
      "metadata": {
        "id": "uQV7JhRl5LSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: implement the predict function\n",
        "# def predict(model: nn.Module, dataloader: DataLoader, device: torch.device) -> List[List[str]]:\n",
        "#     model.eval()\n",
        "#     preds = []\n",
        "#     with torch.no_grad():\n",
        "#         for batch in tqdm(dataloader):\n",
        "#             raise NotImplemented\n",
        "                    \n",
        "#     return preds\n",
        "\n",
        "\n",
        "\n",
        "# v1\n",
        "# def predict(model: nn.Module, dataloader: DataLoader, device: torch.device) -> List[List[str]]:\n",
        "#     model.eval()\n",
        "#     preds = []\n",
        "#     with torch.no_grad():\n",
        "#         for batch in tqdm(dataloader):\n",
        "#             batch = [b.to(device) for b in batch]\n",
        "#             token_ids, mask = batch[:2]\n",
        "#             logits = model(token_ids, mask)\n",
        "#             pred_tag_ids = logits.argmax(dim=-1)\n",
        "#             pred_tags = []\n",
        "#             for i, tag_ids in enumerate(pred_tag_ids):\n",
        "#               #trueLen = \n",
        "#                 tag_labels = []\n",
        "#                 for tag_id in tag_ids:\n",
        "#                     if tag_id == 0:\n",
        "#                         break\n",
        "#                     tag_labels.append(NERDataset.idx2tag[tag_id])\n",
        "#                 pred_tags.append(tag_labels)\n",
        "#             preds.extend(pred_tags)\n",
        "#     return preds\n",
        "\n",
        "\n",
        "\n",
        "def predict(model: nn.Module, dataloader: DataLoader, device: torch.device) -> List[List[str]]:\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            batch = [b.to(device) for b in batch]\n",
        "\n",
        "            token_ids, mask = batch[0:2]\n",
        "            if len(batch) != 2:\n",
        "                tag_ids = batch[2]\n",
        "\n",
        "            output = model(token_ids, mask)\n",
        "            pred_tag_ids = output.argmax(dim=-1)\n",
        "            pred_tags = []\n",
        "            for i, ids in enumerate(pred_tag_ids):\n",
        "                trueLen = len(token_ids[i]) - (mask[i] == 1).sum().item()\n",
        "                tag_labels = [NERDataset.idx2tag[tag_id] for tag_id in ids if tag_id != 0]\n",
        "                pred_tags.append(tag_labels[:trueLen])\n",
        "            preds.extend(pred_tags)\n",
        "    return preds\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:43.207551Z",
          "iopub.execute_input": "2022-02-17T08:25:43.207812Z",
          "iopub.status.idle": "2022-02-17T08:25:43.426052Z",
          "shell.execute_reply.started": "2022-02-17T08:25:43.207774Z",
          "shell.execute_reply": "2022-02-17T08:25:43.425331Z"
        },
        "trusted": true,
        "id": "2BeTuu4i5LSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
        "from conlleval import evaluate"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:43.430210Z",
          "iopub.execute_input": "2022-02-17T08:25:43.431127Z",
          "iopub.status.idle": "2022-02-17T08:25:44.519771Z",
          "shell.execute_reply.started": "2022-02-17T08:25:43.431084Z",
          "shell.execute_reply": "2022-02-17T08:25:44.519005Z"
        },
        "trusted": true,
        "id": "EzFjEe0c5LSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9f49a59-27ba-4bde-b8e4-4002a272724b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-16 04:19:42--  https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7502 (7.3K) [text/plain]\n",
            "Saving to: ‘conlleval.py’\n",
            "\n",
            "\rconlleval.py          0%[                    ]       0  --.-KB/s               \rconlleval.py        100%[===================>]   7.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-03-16 04:19:42 (91.1 MB/s) - ‘conlleval.py’ saved [7502/7502]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use the conlleval script to measure the entity-level f1\n",
        "true_tags = []\n",
        "# trueCurr, predCurr = [],[]\n",
        "# trueTotal, predTotal = [],[]\n",
        "pred_tags = []\n",
        "\n",
        "myPredTags, myTrueTags = [], []\n",
        "\n",
        "for tags in predict(model, val_dataloader, device):\n",
        "    pred_tags.extend(tags)\n",
        "    pred_tags.append('O')\n",
        "    myPredTags.append(tags)\n",
        "    # predCurr.extend(tags)\n",
        "    # predTotal.append(predCurr)\n",
        "\n",
        "\n",
        "for tags in val_raw['tags']:\n",
        "    tempTrueTags = tags.strip().split()\n",
        "    true_tags.extend(tempTrueTags)\n",
        "    true_tags.append('O')\n",
        "    myTrueTags.append(tempTrueTags)\n",
        "\n",
        "    # trueCurr.extend(tags.strip().split())\n",
        "    # trueTotal.append(trueCurr)\n",
        "    \n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:44.521224Z",
          "iopub.execute_input": "2022-02-17T08:25:44.521511Z",
          "iopub.status.idle": "2022-02-17T08:25:45.394177Z",
          "shell.execute_reply.started": "2022-02-17T08:25:44.521470Z",
          "shell.execute_reply": "2022-02-17T08:25:45.393518Z"
        },
        "trusted": true,
        "id": "lVAnQYdD5LSr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94d9cd5f-bef4-4f36-fbfd-427e0992e3ac"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 102/102 [00:18<00:00,  5.57it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = 56\n",
        "#for testing equivalency\n",
        "print(len(myTrueTags[num]), myTrueTags[num])\n",
        "print(len(myPredTags[num]),myPredTags[num])\n",
        "wrongCount = 0\n",
        "for i in range(len(myTrueTags)):\n",
        "    if(myTrueTags[i] != myPredTags[i]):\n",
        "        wrongCount += 1\n",
        "    if(len(myTrueTags[i]) != len(myPredTags[i])):\n",
        "        print(\"len\", i)\n",
        "print(wrongCount, len(myTrueTags))\n"
      ],
      "metadata": {
        "id": "PL_wt3IpMJq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ecfc36-03b1-42f7-a67d-da56fa66c985"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11 ['O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O']\n",
            "11 ['I-PER', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O']\n",
            "1584 3250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(true_tags, pred_tags)"
      ],
      "metadata": {
        "id": "JEE1vHwAMHRq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a181d784-79c6-4f04-9c4a-e0124ebec776"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 54612 tokens with 5942 phrases; found: 5358 phrases; correct: 3764.\n",
            "accuracy:  61.66%; (non-O)\n",
            "accuracy:  93.38%; precision:  70.25%; recall:  63.35%; FB1:  66.62\n",
            "              LOC: precision:  84.92%; recall:  79.10%; FB1:  81.91  1711\n",
            "             MISC: precision:  77.30%; recall:  72.02%; FB1:  74.56  859\n",
            "              ORG: precision:  61.32%; recall:  56.15%; FB1:  58.62  1228\n",
            "              PER: precision:  57.31%; recall:  48.53%; FB1:  52.56  1560\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70.25009331840238, 63.34567485695052, 66.61946902654867)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example output from the above codeblock. We will take the overall test F1 score (69.24 in this example) and grade accordingly.\n",
        "```\n",
        "processed 54612 tokens with 5942 phrases; found: 5554 phrases; correct: 3980.\n",
        "accuracy:  65.78%; (non-O)\n",
        "accuracy:  93.88%; precision:  71.66%; recall:  66.98%; FB1:  69.24\n",
        "              LOC: precision:  84.58%; recall:  77.03%; FB1:  80.63  1673\n",
        "             MISC: precision:  77.31%; recall:  71.69%; FB1:  74.40  855\n",
        "              ORG: precision:  58.71%; recall:  63.83%; FB1:  61.16  1458\n",
        "              PER: precision:  66.84%; recall:  56.89%; FB1:  61.47  1568\n",
        "(71.66006481814908, 66.98081454055873, 69.24147529575504)\n",
        "```\n",
        "If the codeblock above errors out, check your implementation of the `predict` function. It should return a nested list of lists, each containing predicted tags in their IOB string forms."
      ],
      "metadata": {
        "id": "ztKqd9J15LSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Todo Part 4\n",
        "Once you finish all previous todos and are satisfied with the model performance on the validation set, make predictions on the test set and keep a copy of the `submission.txt` file by downloading it to your local machine. You can find `submission.txt` under Output > `/kaggle/working`."
      ],
      "metadata": {
        "id": "IaChyXkY5LSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOU SHOULD NOT CHANGE THIS CODEBLOCK\n",
        "# make prediction on the test set and save to submission.txt\n",
        "preds = predict(model, test_dataloader, device)\n",
        "with open(\"submission.txt\", \"w\") as f:\n",
        "    for tags in preds:\n",
        "        f.write(\" \".join(tags) + \"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:45.395550Z",
          "iopub.execute_input": "2022-02-17T08:25:45.395822Z",
          "iopub.status.idle": "2022-02-17T08:25:46.111937Z",
          "shell.execute_reply.started": "2022-02-17T08:25:45.395787Z",
          "shell.execute_reply": "2022-02-17T08:25:46.111234Z"
        },
        "trusted": true,
        "id": "dVt102qy5LSs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d66c70f3-bc2e-4cf8-f4a2-f46a2597b9c1"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 108/108 [00:19<00:00,  5.68it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:27:24.950886Z",
          "iopub.execute_input": "2022-02-17T08:27:24.951166Z",
          "iopub.status.idle": "2022-02-17T08:27:24.957143Z",
          "shell.execute_reply.started": "2022-02-17T08:27:24.951135Z",
          "shell.execute_reply": "2022-02-17T08:27:24.956414Z"
        },
        "trusted": true,
        "id": "jJOtOKyR5LSs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "35621483-f3ff-4db3-b974-3029b5b0e442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:27:30.109566Z",
          "iopub.execute_input": "2022-02-17T08:27:30.109841Z",
          "iopub.status.idle": "2022-02-17T08:27:30.796831Z",
          "shell.execute_reply.started": "2022-02-17T08:27:30.109811Z",
          "shell.execute_reply": "2022-02-17T08:27:30.795875Z"
        },
        "trusted": true,
        "id": "5VquJfjq5LSs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d9c6cf0-9830-480a-c6eb-36465646c197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "190i-mp2-zmarks  conlleval.py.2  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/    test_tokens.txt\n",
            "conlleval.py     conlleval.py.3  \u001b[01;34msample_data\u001b[0m/    train.csv\n",
            "conlleval.py.1   conlleval.py.4  submission.txt  val.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(model, \"./190i-mp2-zmarks\")"
      ],
      "metadata": {
        "id": "ioUQaKri5LSs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}